{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('speeddating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_null</th>\n",
       "      <th>wave</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>d_age</th>\n",
       "      <th>d_d_age</th>\n",
       "      <th>race</th>\n",
       "      <th>race_o</th>\n",
       "      <th>samerace</th>\n",
       "      <th>...</th>\n",
       "      <th>d_expected_num_interested_in_me</th>\n",
       "      <th>d_expected_num_matches</th>\n",
       "      <th>like</th>\n",
       "      <th>guess_prob_liked</th>\n",
       "      <th>d_like</th>\n",
       "      <th>d_guess_prob_liked</th>\n",
       "      <th>met</th>\n",
       "      <th>decision</th>\n",
       "      <th>decision_o</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'[4-6]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'European/Caucasian-American'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[5-6]'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'[0-1]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'European/Caucasian-American'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[5-6]'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'[0-1]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[0-4]'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'[2-3]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'European/Caucasian-American'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[5-6]'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'[2-3]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'Latino/Hispanic American'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[5-6]'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  has_null  wave     gender   age  age_o  d_age   d_d_age  \\\n",
       "0      b''   1.0  b'female'  21.0   27.0    6.0  b'[4-6]'   \n",
       "1      b''   1.0  b'female'  21.0   22.0    1.0  b'[0-1]'   \n",
       "2      b''   1.0  b'female'  21.0   22.0    1.0  b'[0-1]'   \n",
       "3      b''   1.0  b'female'  21.0   23.0    2.0  b'[2-3]'   \n",
       "4      b''   1.0  b'female'  21.0   24.0    3.0  b'[2-3]'   \n",
       "\n",
       "                                       race  \\\n",
       "0  b'Asian/Pacific Islander/Asian-American'   \n",
       "1  b'Asian/Pacific Islander/Asian-American'   \n",
       "2  b'Asian/Pacific Islander/Asian-American'   \n",
       "3  b'Asian/Pacific Islander/Asian-American'   \n",
       "4  b'Asian/Pacific Islander/Asian-American'   \n",
       "\n",
       "                                     race_o samerace  ...  \\\n",
       "0            b'European/Caucasian-American'     b'0'  ...   \n",
       "1            b'European/Caucasian-American'     b'0'  ...   \n",
       "2  b'Asian/Pacific Islander/Asian-American'     b'1'  ...   \n",
       "3            b'European/Caucasian-American'     b'0'  ...   \n",
       "4               b'Latino/Hispanic American'     b'0'  ...   \n",
       "\n",
       "   d_expected_num_interested_in_me  d_expected_num_matches like  \\\n",
       "0                         b'[0-3]'                b'[3-5]'  7.0   \n",
       "1                         b'[0-3]'                b'[3-5]'  7.0   \n",
       "2                         b'[0-3]'                b'[3-5]'  7.0   \n",
       "3                         b'[0-3]'                b'[3-5]'  7.0   \n",
       "4                         b'[0-3]'                b'[3-5]'  6.0   \n",
       "\n",
       "  guess_prob_liked    d_like  d_guess_prob_liked  met  decision  decision_o  \\\n",
       "0              6.0  b'[6-8]'            b'[5-6]'  0.0      b'1'        b'0'   \n",
       "1              5.0  b'[6-8]'            b'[5-6]'  1.0      b'1'        b'0'   \n",
       "2              NaN  b'[6-8]'            b'[0-4]'  1.0      b'1'        b'1'   \n",
       "3              6.0  b'[6-8]'            b'[5-6]'  0.0      b'1'        b'1'   \n",
       "4              6.0  b'[6-8]'            b'[5-6]'  0.0      b'1'        b'1'   \n",
       "\n",
       "   match  \n",
       "0   b'0'  \n",
       "1   b'0'  \n",
       "2   b'1'  \n",
       "3   b'1'  \n",
       "4   b'1'  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list=['gender', 'age', 'race', 'age_o', 'samerace', 'attractive', 'sincere', 'intelligence', \"funny\", \"ambition_partner\", \"gaming\",'match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>age_o</th>\n",
       "      <th>samerace</th>\n",
       "      <th>attractive</th>\n",
       "      <th>sincere</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>funny</th>\n",
       "      <th>ambition_partner</th>\n",
       "      <th>gaming</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>27.0</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>22.0</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>22.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>23.0</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>24.0</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender   age                                      race  age_o samerace  \\\n",
       "0  b'female'  21.0  b'Asian/Pacific Islander/Asian-American'   27.0     b'0'   \n",
       "1  b'female'  21.0  b'Asian/Pacific Islander/Asian-American'   22.0     b'0'   \n",
       "2  b'female'  21.0  b'Asian/Pacific Islander/Asian-American'   22.0     b'1'   \n",
       "3  b'female'  21.0  b'Asian/Pacific Islander/Asian-American'   23.0     b'0'   \n",
       "4  b'female'  21.0  b'Asian/Pacific Islander/Asian-American'   24.0     b'0'   \n",
       "\n",
       "   attractive  sincere  intelligence  funny  ambition_partner  gaming match  \n",
       "0         6.0      8.0           8.0    8.0               6.0     1.0  b'0'  \n",
       "1         6.0      8.0           8.0    8.0               5.0     1.0  b'0'  \n",
       "2         6.0      8.0           8.0    8.0               5.0     1.0  b'1'  \n",
       "3         6.0      8.0           8.0    8.0               6.0     1.0  b'1'  \n",
       "4         6.0      8.0           8.0    8.0               6.0     1.0  b'1'  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                0\n",
       "age                  95\n",
       "race                  0\n",
       "age_o               104\n",
       "samerace              0\n",
       "attractive          105\n",
       "sincere             105\n",
       "intelligence        105\n",
       "funny               105\n",
       "ambition_partner    712\n",
       "gaming               79\n",
       "match                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambition_partner    8.498448\n",
      "attractive          1.253282\n",
      "sincere             1.253282\n",
      "intelligence        1.253282\n",
      "funny               1.253282\n",
      "age_o               1.241346\n",
      "age                 1.133922\n",
      "gaming              0.942946\n",
      "gender              0.000000\n",
      "race                0.000000\n",
      "samerace            0.000000\n",
      "match               0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "null_percentage = df.isnull().sum() / len(df) * 100\n",
    "null_percentage = null_percentage.sort_values(ascending=False)\n",
    "print(null_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = null_percentage[null_percentage > 5].index.tolist()\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dropped observations: -2.95%\n"
     ]
    }
   ],
   "source": [
    "dropped_percentage = (1 - len(df) / len(data)) * 100\n",
    "print(f\"Percentage of dropped observations: {dropped_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>attractive</th>\n",
       "      <th>sincere</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>funny</th>\n",
       "      <th>gaming</th>\n",
       "      <th>gender_b'female'</th>\n",
       "      <th>gender_b'male'</th>\n",
       "      <th>race_b'Asian/Pacific Islander/Asian-American'</th>\n",
       "      <th>race_b'Black/African American'</th>\n",
       "      <th>race_b'European/Caucasian-American'</th>\n",
       "      <th>race_b'Latino/Hispanic American'</th>\n",
       "      <th>race_b'Other'</th>\n",
       "      <th>samerace_b'0'</th>\n",
       "      <th>samerace_b'1'</th>\n",
       "      <th>match_b'0'</th>\n",
       "      <th>match_b'1'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>25.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>25.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>25.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8138 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  age_o  attractive  sincere  intelligence  funny  gaming  \\\n",
       "0     21.0   27.0         6.0      8.0           8.0    8.0     1.0   \n",
       "1     21.0   22.0         6.0      8.0           8.0    8.0     1.0   \n",
       "2     21.0   22.0         6.0      8.0           8.0    8.0     1.0   \n",
       "3     21.0   23.0         6.0      8.0           8.0    8.0     1.0   \n",
       "4     21.0   24.0         6.0      8.0           8.0    8.0     1.0   \n",
       "...    ...    ...         ...      ...           ...    ...     ...   \n",
       "8373  25.0   26.0         8.0      7.0           6.0    7.0     1.0   \n",
       "8374  25.0   24.0         8.0      7.0           6.0    7.0     1.0   \n",
       "8375  25.0   29.0         8.0      7.0           6.0    7.0     1.0   \n",
       "8376  25.0   22.0         8.0      7.0           6.0    7.0     1.0   \n",
       "8377  25.0   22.0         8.0      7.0           6.0    7.0     1.0   \n",
       "\n",
       "      gender_b'female'  gender_b'male'  \\\n",
       "0                    1               0   \n",
       "1                    1               0   \n",
       "2                    1               0   \n",
       "3                    1               0   \n",
       "4                    1               0   \n",
       "...                ...             ...   \n",
       "8373                 0               1   \n",
       "8374                 0               1   \n",
       "8375                 0               1   \n",
       "8376                 0               1   \n",
       "8377                 0               1   \n",
       "\n",
       "      race_b'Asian/Pacific Islander/Asian-American'  \\\n",
       "0                                                 1   \n",
       "1                                                 1   \n",
       "2                                                 1   \n",
       "3                                                 1   \n",
       "4                                                 1   \n",
       "...                                             ...   \n",
       "8373                                              0   \n",
       "8374                                              0   \n",
       "8375                                              0   \n",
       "8376                                              0   \n",
       "8377                                              0   \n",
       "\n",
       "      race_b'Black/African American'  race_b'European/Caucasian-American'  \\\n",
       "0                                  0                                    0   \n",
       "1                                  0                                    0   \n",
       "2                                  0                                    0   \n",
       "3                                  0                                    0   \n",
       "4                                  0                                    0   \n",
       "...                              ...                                  ...   \n",
       "8373                               0                                    1   \n",
       "8374                               0                                    1   \n",
       "8375                               0                                    1   \n",
       "8376                               0                                    1   \n",
       "8377                               0                                    1   \n",
       "\n",
       "      race_b'Latino/Hispanic American'  race_b'Other'  samerace_b'0'  \\\n",
       "0                                    0              0              1   \n",
       "1                                    0              0              1   \n",
       "2                                    0              0              0   \n",
       "3                                    0              0              1   \n",
       "4                                    0              0              1   \n",
       "...                                ...            ...            ...   \n",
       "8373                                 0              0              1   \n",
       "8374                                 0              0              1   \n",
       "8375                                 0              0              1   \n",
       "8376                                 0              0              1   \n",
       "8377                                 0              0              1   \n",
       "\n",
       "      samerace_b'1'  match_b'0'  match_b'1'  \n",
       "0                 0           1           0  \n",
       "1                 0           1           0  \n",
       "2                 1           0           1  \n",
       "3                 0           0           1  \n",
       "4                 0           0           1  \n",
       "...             ...         ...         ...  \n",
       "8373              0           1           0  \n",
       "8374              0           1           0  \n",
       "8375              0           1           0  \n",
       "8376              0           1           0  \n",
       "8377              0           1           0  \n",
       "\n",
       "[8138 rows x 18 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (one feature): 0.8347665847665847\n",
      "Cross-validation scores (one feature): [0.83660934 0.83660934 0.83660934 0.83712354 0.83650891]\n",
      "Mean cross-validation score (one feature): 0.8366920924388654\n"
     ]
    }
   ],
   "source": [
    "X = data[['attractive']]\n",
    "y = data[\"match_b'1'\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "score = logreg.score(X_test, y_test)\n",
    "\n",
    "cv_scores = cross_val_score(logreg, X, y, cv=5)\n",
    "\n",
    "print('Accuracy (one feature):', score)\n",
    "print('Cross-validation scores (one feature):', cv_scores)\n",
    "print('Mean cross-validation score (one feature):', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (one feature): 0.8347665847665847\n",
      "Cross-validation scores (one feature): [0.83660934 0.83660934 0.83660934 0.83712354 0.83650891]\n",
      "Mean cross-validation score (one feature): 0.8366920924388654\n"
     ]
    }
   ],
   "source": [
    "X = data[[\"race_b'Latino/Hispanic American'\"]]\n",
    "y = data[\"match_b'1'\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "score = logreg.score(X_test, y_test)\n",
    "\n",
    "cv_scores = cross_val_score(logreg, X, y, cv=5)\n",
    "\n",
    "print('Accuracy (one feature):', score)\n",
    "print('Cross-validation scores (one feature):', cv_scores)\n",
    "print('Mean cross-validation score (one feature):', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (one feature): 0.8347665847665847\n",
      "Cross-validation scores (one feature): [0.83660934 0.83660934 0.83660934 0.83712354 0.83650891]\n",
      "Mean cross-validation score (one feature): 0.8366920924388654\n"
     ]
    }
   ],
   "source": [
    "X = data[['age']]\n",
    "y = data[\"match_b'1'\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "score = logreg.score(X_test, y_test)\n",
    "\n",
    "cv_scores = cross_val_score(logreg, X, y, cv=5)\n",
    "\n",
    "print('Accuracy (one feature):', score)\n",
    "print('Cross-validation scores (one feature):', cv_scores)\n",
    "print('Mean cross-validation score (one feature):', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (one feature): 0.8347665847665847\n",
      "Cross-validation scores (one feature): [0.83660934 0.83660934 0.83660934 0.83712354 0.83650891]\n",
      "Mean cross-validation score (one feature): 0.8366920924388654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X = data.drop([\"match_b'0'\",\"match_b'1'\"\t], axis =1) \n",
    "y = data[\"match_b'1'\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "score = logreg.score(X_test, y_test)\n",
    "\n",
    "cv_scores = cross_val_score(logreg, X, y, cv=5)\n",
    "\n",
    "print('Accuracy (one feature):', score)\n",
    "print('Cross-validation scores (one feature):', cv_scores)\n",
    "print('Mean cross-validation score (one feature):', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      attractive\n",
      "0            6.0\n",
      "1            6.0\n",
      "2            6.0\n",
      "3            6.0\n",
      "4            6.0\n",
      "...          ...\n",
      "8373         8.0\n",
      "8374         8.0\n",
      "8375         8.0\n",
      "8376         8.0\n",
      "8377         8.0\n",
      "\n",
      "[8138 rows x 1 columns]\n",
      "0       1\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "8373    1\n",
      "8374    1\n",
      "8375    1\n",
      "8376    1\n",
      "8377    1\n",
      "Name: match_b'0', Length: 8138, dtype: uint8\n",
      "Accuracy (one feature): 0.8347665847665847\n",
      "Cross-validation scores (one feature): [0.83660934 0.83660934 0.83660934 0.83712354 0.83650891]\n",
      "Mean cross-validation score (one feature): 0.8366920924388654\n"
     ]
    }
   ],
   "source": [
    "X = data[['attractive']]\n",
    "y = data[\"match_b'0'\"]\n",
    "print (X)\n",
    "print (y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "score = logreg.score(X_test, y_test)\n",
    "\n",
    "cv_scores = cross_val_score(logreg, X, y, cv=5)\n",
    "\n",
    "print('Accuracy (one feature):', score)\n",
    "print('Cross-validation scores (one feature):', cv_scores)\n",
    "print('Mean cross-validation score (one feature):', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[['attractive']]\n",
    "y = data[\"match_b'0'\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "ypredict= logreg.predict(X_test)\n",
    "\n",
    "ypredict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  269],\n",
       "       [   0, 1359]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, ypredict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 1s: 16.330793806832144\n",
      "Percentage of 0s: 83.66920619316785\n"
     ]
    }
   ],
   "source": [
    "match_counts = data[\"match_b'1'\"].value_counts(normalize=True)\n",
    "print('Percentage of 1s:', match_counts[1]*100)\n",
    "print('Percentage of 0s:', match_counts[0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8347665847665847\n",
      "Cross-validation scores: [0.81142506 0.83660934 0.83538084 0.83712354 0.83650891]\n",
      "Mean cross-validation score: 0.8314095371563102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop([\"match_b'0'\",\"match_b'1'\"\t], axis =1) \n",
    "y = data['match_b\\'1\\'']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "score = dt.score(X_test, y_test)\n",
    "\n",
    "cv_scores = cross_val_score(dt, X, y, cv=5)\n",
    "\n",
    "print('Accuracy:', score)\n",
    "print('Cross-validation scores:', cv_scores)\n",
    "print('Mean cross-validation score:', cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Best accuracy score: 0.837\n",
      "Accuracy on testing data: 0.835\n"
     ]
    }
   ],
   "source": [
    "X = data.drop([\"match_b'0'\", \"match_b'1'\"], axis=1)\n",
    "y = data[\"match_b'1'\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "score = grid_search.score(X_test, y_test)\n",
    "print(f\"Accuracy on testing data: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: 0.8371735791090631\n",
      "Test accuracy: 0.8347665847665847\n"
     ]
    }
   ],
   "source": [
    "df_text = data.drop([\"match_b'0'\", \"match_b'1'\"], axis=1).astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Vectorize the data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_text).toarray()\n",
    "y = data[\"match_b'1'\"].to_numpy()\n",
    "\n",
    "# Compute the kernel matrix\n",
    "K = cosine_similarity(X)\n",
    "\n",
    "# Train and evaluate the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "K_train = K[X_train[:, 0].astype(int), :][:, X_train[:, 0].astype(int)]\n",
    "K_test = K[X_test[:, 0].astype(int), :][:, X_train[:, 0].astype(int)]\n",
    "\n",
    "svm = SVC(kernel='precomputed')\n",
    "svm.fit(K_train, y_train)\n",
    "scores = cross_val_score(svm, K_train, y_train, cv=5)\n",
    "\n",
    "print('Cross-validation scores:', scores.mean())\n",
    "print('Test accuracy:', svm.score(K_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Model  Cross-validation scores  Test accuracy\n",
      "0                      Decision Tree                 0.831410       0.834767\n",
      "1                      Random Forest                 0.836201       0.834767\n",
      "2  SVM with cosine similarity kernel                 0.837174       0.834767\n"
     ]
    }
   ],
   "source": [
    "X_dt = data.drop([\"match_b'0'\", \"match_b'1'\"], axis=1) \n",
    "y_dt = data[\"match_b'1'\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dt, y_dt, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "score_dt = dt.score(X_test, y_test)\n",
    "cv_scores_dt = cross_val_score(dt, X_dt, y_dt, cv=5)\n",
    "\n",
    "# Random forest model\n",
    "X_rf = data.drop([\"match_b'0'\", \"match_b'1'\"], axis=1) \n",
    "y_rf = data[\"match_b'1'\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "score_rf = grid_search.score(X_test, y_test)\n",
    "cv_scores_rf = cross_val_score(grid_search.best_estimator_, X_rf, y_rf, cv=5)\n",
    "\n",
    "# SVM model\n",
    "df_text = data.drop([\"match_b'0'\", \"match_b'1'\"], axis=1).astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_svm = vectorizer.fit_transform(df_text).toarray()\n",
    "y_svm = data[\"match_b'1'\"].to_numpy()\n",
    "\n",
    "K = cosine_similarity(X_svm)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_svm, y_svm, test_size=0.2, random_state=42)\n",
    "K_train = K[X_train[:, 0].astype(int), :][:, X_train[:, 0].astype(int)]\n",
    "K_test = K[X_test[:, 0].astype(int), :][:, X_train[:, 0].astype(int)]\n",
    "\n",
    "svm = SVC(kernel='precomputed')\n",
    "svm.fit(K_train, y_train)\n",
    "scores_svm = cross_val_score(svm, K_train, y_train, cv=5)\n",
    "score_svm = svm.score(K_test, y_test)\n",
    "\n",
    "# Create a table with the model results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Random Forest', 'SVM with cosine similarity kernel'],\n",
    "    'Cross-validation scores': [cv_scores_dt.mean(), cv_scores_rf.mean(), scores_svm.mean()],\n",
    "    'Test accuracy': [score_dt, score_rf, score_svm]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '21.0 27.0 6.0 8.0 8.0 8.0 1.0 1 0 1 0 0 0 0 1 0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m df_text \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdrop([\u001b[39m\"\u001b[39m\u001b[39mmatch_b\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmatch_b\u001b[39m\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(x), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Compute the kernel matrix using the tree kernel\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m K \u001b[39m=\u001b[39m pairwise_kernels(df_text, metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msigmoid\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m      8\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(K, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2205\u001b[0m, in \u001b[0;36mpairwise_kernels\u001b[1;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   2202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2203\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown kernel \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m metric)\n\u001b[1;32m-> 2205\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1579\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1576\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1578\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 1579\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m   1581\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1274\u001b[0m, in \u001b[0;36msigmoid_kernel\u001b[1;34m(X, Y, gamma, coef0)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msigmoid_kernel\u001b[39m(X, Y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, gamma\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, coef0\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1249\u001b[0m     \u001b[39m\"\"\"Compute the sigmoid kernel between X and Y.\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \n\u001b[0;32m   1251\u001b[0m \u001b[39m        K(X, Y) = tanh(gamma <X, Y> + coef0)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[39m        Sigmoid kernel between two arrays.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1274\u001b[0m     X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1275\u001b[0m     \u001b[39mif\u001b[39;00m gamma \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m         gamma \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:146\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m     dtype \u001b[39m=\u001b[39m dtype_float\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m Y \u001b[39mis\u001b[39;00m X \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    147\u001b[0m         X,\n\u001b[0;32m    148\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    149\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    150\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    151\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    152\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    156\u001b[0m         X,\n\u001b[0;32m    157\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    162\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '21.0 27.0 6.0 8.0 8.0 8.0 1.0 1 0 1 0 0 0 0 1 0'"
     ]
    }
   ],
   "source": [
    "# Convert the data to strings\n",
    "df_text = data.drop([\"match_b'0'\", \"match_b'1'\"], axis=1).astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Compute the kernel matrix using the tree kernel\n",
    "K = pairwise_kernels(df_text, metric='sigmoid')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(K, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVM model using the tree kernel\n",
    "svm = SVC(kernel='precomputed')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "scores = cross_val_score(svm, X_train, y_train, cv=5)\n",
    "\n",
    "print('Cross-validation scores:', scores.mean())\n",
    "print('Test accuracy:', svm.score(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
